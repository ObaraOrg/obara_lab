## NOTES:
## To run it you need to first specify the lab group to run it on
## example: qsub -g tga-oba2 TSUBAME_run_example.qsub

## This first part is the post bash command for the qsub engine
#!/bin/sh
#$-cwd    					## work in the current directory 
#$ -V	  					## use the present envirorment variables (picks up serpent)
#$ -N serpent_ex			## name of the run
#$ -o output&err_file.out	## write output to a file
#$ -e output&err_file.out	## write qsub errors to a file (here it's the same,easier to read)
## Resource type F: qty 4 (this is specific to the TSUBAME, more on this check the website)
## https://helpdesk.t3.gsic.titech.ac.jp/manuals/handbook.en/jobs/#job_flow
#$ -l f_node=4				## resource type to use
#$ -l h_rt=02:00:00			## how much time to use the resource (here its 2h)	

## Before running serpent or any program you need to load some modules, prerequisite as to make it run properly
## Load modules
module load gcc
module load cuda
module load intel
module load openmpi 
# module load intel-mpi

## This is the actual bash part you run

## Example running with the OPENMPI module (openmpi)
## Example of 2 process per reserved node (-npernode), the total of MPI process (-n) is 8 (2x f_node=4)
## 		the TSUBAME CPU has 28 cores per node so the number of threads (-omp) should be the
## 		number of cores per node divided by the number of MPI processes requested per node, 28/2=14
mpirun -npernode 2 -n 8 -x LD_LIBRARY_PATH sss2 -omp 14 SERPENT_file

## Example running with the INTEL MPI module (intel-mpi)
# mpiexec.hydra -ppn 1 -n 4 sss2 -omp 26 reactor0